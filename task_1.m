% Neural Networks Assignment 1
% By: Pepijn van Heiningen (s1045105) & Michiel Vos (???)

function task_1(filename)
    %Find, for each sample, the most likely density function that was used for generating the sample and the most likely value of the parameter s.
    disp(char(10));   
    disp('-------------------------------------------------------');
    disp('Neural Networks - Assignment 1');
    disp('By: Pepijn van Heiningen (s1045105) & Michiel Vos (???)');
    disp('-------------------------------------------------------');    
    load(filename);
    counter = 1;
    results = zeros(1000,4,4); %s, function, dataset
    data = [A B C D];
    functions = {@normal, @par, @rec, @tri};
    functionname = {'normal' 'parabolic' 'rectangular' 'triangular'};
    for dataset = 1:4
        for s = 0.5:0.001:1.5
            for func = 1:4
                results(counter,func,dataset) = log_likelihood(data(:,dataset), functions{func}, s);
            end
            counter = counter + 1;
        end
        counter = 1;
    end
    [max1, j] = max(results);
    [max2, i] = max(max1(:,:,:));
    disp(char(10));    
    disp('Part 1:');
    disp(['Sample A is (most likely) generated by the ' functionname{i(:,:,1)} ' density function with parameter s ' sprintf('%i', 0.5+0.001*j(:,i(:,:,1),1)) '.']);
    disp(['Sample B is (most likely) generated by the ' functionname{i(:,:,2)} ' density function with parameter s ' sprintf('%i', 0.5+0.001*j(:,i(:,:,2),2)) '.']);
    disp(['Sample C is (most likely) generated by the ' functionname{i(:,:,3)} ' density function with parameter s ' sprintf('%i', 0.5+0.001*j(:,i(:,:,3),3)) '.']);
    disp(['Sample D is (most likely) generated by the ' functionname{i(:,:,4)} ' density function with parameter s ' sprintf('%i', 0.5+0.001*j(:,i(:,:,4),4)) '.']);
    
    %Demonstrate (numerically or by a formal proof) that the area under the parabola (par.m) is always 1, independently on the value of s.
    fun = @(x,s) par(x,s);
    counter = 1;
    for i=0.01:0.01:100
        output(counter) = quadgk(@(x)fun(x,i),-i,i);
        counter = counter + 1;
    end
    disp(char(10));
    disp('Part 2:');
    disp(['Mean: ' sprintf('%f', mean(output))])
    disp(['Std: ' sprintf('%f', std(output))])
    
    %Estimate all six parameters of the corresponding Gaussian mixture of two normal distributions (i.e., pA, pB, µA, µB, ?A, and ?B) using three different methods:
    %METHOD 1: Directly calculate the parameters
    load('mix2.mat')
    values = x;
    classification = class(:);
    counterA = 1;
    counterB = 1;
    for i=1:size(x)
        if(class(i)=='A')
            classA(counterA) = values(i);
            counterA = counterA + 1;
        else
            classB(counterB) = values(i);
            counterB = counterB + 1;
        end
    end
    
    disp(char(10));
    disp('Part 3:');
    disp('1:');
    disp(['pA: ' sprintf('%f', size(classA')/size(classification(:))) ', pB: ' sprintf('%f', size(classB')/size(classification(:))) ', µA: ' sprintf('%f', mean(classA)) ', µB: ' sprintf('%f', mean(classB)) ', sA: ' sprintf('%f', std(classA,1)) ', and sB: ' sprintf('%f', std(classB,1))]);
    
    %METHOD 2: Try to estimate the parameters with brute-force
    disp('2:');
    iterations = ((0.5-0.1)/0.01) * ((65-40)/0.1) * ((65-40)/0.1)  * ((4-1)/0.01) * ((4-1)/0.01);
    disp(['Number of iterations: ' sprintf('%i', iterations)]);
    counter = 1;
    done = false;
    tic
    for pA=0.1:0.01:0.5 
        for muA=40:0.1:65
            for muB=40:0.1:65
                for sigmaA=1:0.01:4
                    for sigmaB=1:0.01:4
                        ll(values,pA,1-pA,muA,muB,sigmaA,sigmaB);
                        counter = counter + 1;
                        if(counter == 10000)
                            done = true;
                            time = toc;
                            disp(['Time to complete for-loops: ' sprintf('%i', (iterations/10000)*time) ' seconds.']);
                            break;
                        end
                    end
                    if done
                        break;
                    end
                end
                if done
                    break;
                end                
            end
            if done
                break;
            end            
        end
        if done
            break;
        end        
    end
    %METHOD 3: Use the EM-algorithm to find the optimal values of mixture model parameters
    disp('3:'); 
    %Create a Gaussian mixture model
    mix = gmm(1, 2, 'spherical');

    options = foptions;
    options(1)  = -1;	    
    options(14) = 10;  
    %Initialize Gaussian mixture model
    mix = gmminit(mix, values, options);
    
    options = zeros(1,18);
    options(1)  = 1;	    
    options(14) = 20;    %VRAGEN! Hij doet het niet als je maar 10 iteraties doet
    %Use EM to estimate the parameters of our Gaussian mixture model
    tic
    [mix, options, errlog] = gmmem(mix, values, options);
    toc
    disp(['Centres: ' num2str(mix.centres(1)) '    ' num2str(mix.centres(2))]);
    disp(['Sigmas: ' num2str(sqrt(mix.covars))]);
    disp(['Priors: ' num2str(mix.priors)]);
end

function chance = log_likelihood(sample,distr,s)
    chance = sum(log(distr(sample,s)));
end

function chance = ll(x, pA, pB, muA, muB, sigmaA, sigmaB)
    %VRAGEN of dit klopt
    chance = sum(log(pA*normpdf(x,muA,sigmaA)+pB*normpdf(x,muB,sigmaB)));
end
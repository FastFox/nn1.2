
%
% Stel je wilt het C++-programma iets.cc mooi printen,
% en wellicht er nog wat begeleidende tekst bij schrijven.
%

\documentclass{article}

\setlength{\textheight}{25.7cm}
\setlength{\textwidth}{16cm}
\setlength{\unitlength}{1mm}
\setlength{\topskip}{2.5truecm}
\topmargin 260mm \advance \topmargin -\textheight 
\divide \topmargin by 2 \advance \topmargin -1in 
\headheight 0pt \headsep 0pt \leftmargin 210mm \advance
\leftmargin -\textwidth 
\divide \leftmargin by 2 \advance \leftmargin -1in 
\oddsidemargin \leftmargin \evensidemargin \leftmargin
\parindent=12pt

\frenchspacing
\newcommand{\SA}{Simulated Annealing }

\usepackage[english]{babel}
\usepackage{amsmath}

\usepackage{listings}
% Er zijn talloze parameters ...
\lstset{language=C++, showstringspaces=false, basicstyle=\small,
  numbers=left, numberstyle=\tiny, numberfirstline=false,
  stepnumber=1, tabsize=4, 
  commentstyle=\ttfamily, identifierstyle=\ttfamily,
  stringstyle=\itshape}

\title{Neural Networks: Assignment 1}
\author{Pepijn van Heiningen \& Michiel Vos}

\begin{document}

\maketitle

\section{Introduction}
For the first assignment of the Neural Networks course there were two tasks:
\begin{itemize}
\item Parameter Estimation
\item Classification of handwritten digits
\end{itemize}

\subsection{Task 1}
The first task consists of $3$ different parts. For the first part we were given four samples and four density functions. The task was to find the most likely density function that was used to generate a sample. 
The second part asked to demonstrate that the area under the parabolic density function was always equal to $1$, indepentendly on the value of the input parameter \verb+s+.
Finally we were given a dataset which corresponded to a Gaussian mixture of two normal distributions. The assignment was to find the correct values for the parameters using three different methods.

\subsection{Task 2}

\section{Task 1: Parameter Estimation}
\subsection{Problem Description}
We were given 4 datasets (A, B, C and D) and 4 density functions (Normal, Parabolic, Triangular and Rectangular). The task was to find out what dataset was most likely generated by a particular density function. 

Subsequently we had to demonstrate that the area under the parabola was always $1$.

Finally we were given another dataset, a Gaussian mixture of two normal distributions. We were asked to estimate the six different parameters ($p_A, p_B, \mu_A, \mu_B, \sigma_A, \sigma_B$) using three different methods:

\begin{itemize}
\item By directly calulating the values of the parameters.
\item By trying to brute-force the correct values.
\item By using the Expectation Maximization algorithm.
\end{itemize} 

\subsection{Problem Solution}
To solve the first part of task $1$, we wrote a function that estimated the log likelihood that a sample came from a given distribution. We applied it to all four samples, ranging the parameter \verb+s+ between $0,5$ and $1,5$ with a stepsize of $0,001$, to find the most likely distribution type. In table \ref{table:data1} you can find the results. \\

\begin{table}[!h]
	\begin{center}
		\begin{tabular}{l|l|l}
			Dataset & Density function & \verb+s+ \\
			\hline
			\verb+A+ & Parabolic   & 0,79  \\
			\verb+B+ & Normal  & 0,54 \\
			\verb+C+ & Rectangular   & 1,09   \\
			\verb+D+ & Triangular   & 0,99   \\
		\end{tabular}
		\caption{Datasets and most likely density functions}
		\label{table:data1}
	\end{center}
\end{table}

To demonstrate that the area under the parabolic density function was always $1$, we calculated the area under the curve for input values of \verb+s+ between $0,01$ and $100$ with a stepsize of $0,01$. By showing that the mean of these values is $1$ and the standard deviation $0$, we know that all output values were equal to $1$.

First we directly calculated the six parameters by using the class variable. The outcome is in table \ref{table:data2}. \\\\

\begin{table}[!h]
	\begin{center}
		\begin{tabular}{l|l|l|l|l|l}
			$p_A$ & $p_B$ & $\mu_A$ & $\mu_B$ & $\sigma_A$ & $\sigma_B$ \\
			\hline
			0,63 & 0,37 & 46,81 & 63,63 & 3,67 & 1,18 \\
		\end{tabular}
		\caption{Calculated parameters and their values}
		\label{table:data2}
	\end{center}
\end{table}

Subsequently we tried the brute-force technique to find the optimal values. The number of combinations was: $2,25 \cdot 10^{11}$. In total it would take $5,77 \cdot 10^6$ seconds or approximately $66,8$ days to brute-force on an Intel Core $i5$-$3470$.\\\\

Finally we used the EM-algorithm to find the optimal values of the mixture parameters. (See table \ref{table:data3}) First we initialized the Gaussian mixture model by using the k-means algorithm for $10$ iterations. Then we trained it using the given dataset for another $10$ generations. Training the model for $10$ iterations took  $0.004762$ seconds, so this approach is approximately $4.7 \cdot 10^{13}$ times faster than the brute-force approach. If we compare the values found with the ``true'' model, we see that there are no differences to $2$ decimals behind the comma. 

\begin{table}[!h]
	\begin{center}
		\begin{tabular}{l|l|l|l|l|l}
			$p_A$ & $p_B$ & $\mu_A$ & $\mu_B$ & $\sigma_A$ & $\sigma_B$ \\
			\hline
			0,63 & 0,37 & 46,81 & 63,63 & 3,67 & 1,18 \\
		\end{tabular}
		\caption{Approximated parameters and their values}
		\label{table:data3}
	\end{center}
\end{table}


\section{Task 2: ClassiÔ¨Åcation of handwritten digits}
\subsection{Problem Description}
The goal of task 2 is to recognize hand-written digits. There are two sets available with images of the hand-written digits, with the labels known. So, this is a superviced learning problem. The first set is the training set. With this data we are going to make a model. The model can be checked with the test set. If the model also performs good on the test set, it indicates the model isn't overfit or overtrained. There are multiple solutions to this problem.

\subsection{Problem Solutions}
Distance-based classifier
An image exists out of 256 points. Those points or pixels have a value. Based on these values we can say something about an image, so called feature. First we are going to load all the images of digit 0 of the training set and extract the features. Than we can calculate the average values. We repeat this for every digit. For a following image, where we don't know the label of, we extract also the features and compare them with the values of the averages we calculate in the step before. The one with the least distance will be probably the correct digit.

The features are the center, the diameter, the radius and the standard deviation of distances from the center, but more and better features can be implemented. 

Perceptron Algorithm
A perceptron is a small neural network. It's based on inputs, weights and outputs. The inputs are the pixels of a image. With the correct weight it's possible to predict the right output. The weights gives importance to a specific pixel. In our case there is just one output node which can have two values. 0 or 1, or true and false. We built 10 models and every model gives an answer to the question "Is this hand-written image a *digit*?" So after checking an image with the 10 models, we can say which number the digit is. 

Training the models costs a lot of time. In our setup we used just 40 training images and we used 10 iterations. It took 72 seconds to train and 58\% of the test set predicted the right answer. 



\end{document}

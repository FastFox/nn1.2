
%
% Stel je wilt het C++-programma iets.cc mooi printen,
% en wellicht er nog wat begeleidende tekst bij schrijven.
%

\documentclass{article}

\setlength{\textheight}{25.7cm}
\setlength{\textwidth}{16cm}
\setlength{\unitlength}{1mm}
\setlength{\topskip}{2.5truecm}
\topmargin 260mm \advance \topmargin -\textheight 
\divide \topmargin by 2 \advance \topmargin -1in 
\headheight 0pt \headsep 0pt \leftmargin 210mm \advance
\leftmargin -\textwidth 
\divide \leftmargin by 2 \advance \leftmargin -1in 
\oddsidemargin \leftmargin \evensidemargin \leftmargin
\parindent=12pt

\frenchspacing
\newcommand{\SA}{Simulated Annealing }

\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{float}
\restylefloat{table}

\usepackage{listings}
% Er zijn talloze parameters ...
\lstset{language=C++, showstringspaces=false, basicstyle=\small,
  numbers=left, numberstyle=\tiny, numberfirstline=false,
  stepnumber=1, tabsize=4, 
  commentstyle=\ttfamily, identifierstyle=\ttfamily,
  stringstyle=\itshape}

\title{Neural Networks: Assignment 1}
\author{Pepijn van Heiningen \& Michiel Vos}

\begin{document}

\maketitle

\section{Introduction}
For the first assignment of the Neural Networks course there were two tasks:
\begin{itemize}
\item Parameter Estimation
\item Classification of handwritten digits
\end{itemize}

\subsection{Task 1}
The first task consists of $3$ different parts. For the first part we were given four samples and four density functions. The first task was to find the most likely density function that was used to generate a sample. 
The second part asked to demonstrate that the area under the parabolic density function was always equal to $1$, indepentendly on the value of the input parameter \verb+s+.
Finally we were given a dataset which corresponded to a Gaussian mixture of two normal distributions. The assignment was to find the correct values for the parameters using three different methods.

\subsection{Task 2}

\section{Task 1: Parameter Estimation}
\subsection{Problem Description}
We were given 4 datasets (A, B, C and D) and 4 density functions (Normal, Parabolic, Triangular and Rectangular). The task was to find out what dataset was most likely generated by a particular density function. 

Subsequently we had to demonstrate that the area under the parabola was always $1$.

Finally we were given another dataset, a Gaussian mixture of two normal distributions. We were asked to estimate the six different parameters ($p_A, p_B, \mu_A, \mu_B, \sigma_A, \sigma_B$) using three different methods:

\begin{itemize}
\item By directly calulating the values of the parameters.
\item By trying to brute-force the correct values.
\item By using the Expectation Maximization algorithm.
\end{itemize} 
\newpage
\subsection{Problem Solution}
To solve the first part of task $1$, we wrote a function that estimated the log likelihood that a sample came from a given distribution. 
The log-likelihood can be calculated by:\\
\[\sum_{x \in X} \ln {p(x|\theta)}\]
We applied it to all four samples, ranging the parameter \verb+s+ between $0,5$ and $1,5$ with a stepsize of $0,001$, to find the most likely distribution type. In table \ref{table:data1} you can find the results. \\

\begin{table}[H]
	\begin{center}
		\begin{tabular}{l|l|l}
			Dataset & Density function & \verb+s+ \\
			\hline
			\verb+A+ & Parabolic   & 0,79  \\
			\verb+B+ & Normal  & 0,54 \\
			\verb+C+ & Rectangular   & 1,09   \\
			\verb+D+ & Triangular   & 0,99   \\
		\end{tabular}
		\caption{Datasets and most likely density functions}
		\label{table:data1}
	\end{center}
\end{table}

To demonstrate that the area under the parabolic density function was always $1$, we calculated the area under the curve for input values of \verb+s+ between $0,01$ and $100$ with a stepsize of $0,01$. By showing that the mean of these values is $1$ and the standard deviation $0$, we know that all output values were equal to $1$.\\

For the third part we calculated the six parameters directly by using the class variable. The results are in table \ref{table:data2}. \\\\

\begin{table}[H]
	\begin{center}
		\begin{tabular}{l|l|l|l|l|l}
			$p_A$ & $p_B$ & $\mu_A$ & $\mu_B$ & $\sigma_A$ & $\sigma_B$ \\
			\hline
			0,63 & 0,37 & 46,81 & 63,63 & 3,67 & 1,18 \\
		\end{tabular}
		\caption{Calculated parameters and their values}
		\label{table:data2}
	\end{center}
\end{table}

Subsequently we tried the brute-force technique to find the optimal values. The number of combinations was: $2,25 \cdot 10^{11}$. In total it would take $5,77 \cdot 10^6$ seconds or approximately $66,8$ days to brute-force on an Intel Core $i5$-$3470$.\\\\

Finally we used the EM-algorithm to find the optimal values of the mixture parameters. (See table \ref{table:data3}) First we initialized the Gaussian mixture model by using the k-means algorithm for $10$ iterations. Then we trained it using the given dataset for another $10$ generations. Training the model for $10$ iterations took  $0.004762$ seconds, so this approach is approximately $4.7 \cdot 10^{13}$ times faster than the brute-force approach. If we compare the values found with the ``true'' model, we see that there are no differences to $2$ decimals behind the comma. 

\begin{table}[!h]
	\begin{center}
		\begin{tabular}{l|l|l|l|l|l}
			$p_A$ & $p_B$ & $\mu_A$ & $\mu_B$ & $\sigma_A$ & $\sigma_B$ \\
			\hline
			0,63 & 0,37 & 46,81 & 63,63 & 3,67 & 1,18 \\
		\end{tabular}
		\caption{Approximated parameters and their values}
		\label{table:data3}
	\end{center}
\end{table}


\section{Task 2: ClassiÔ¨Åcation of handwritten digits}
\subsection{Problem Description}
The goal of task 2 is to recognize hand-written digits. There are two sets available with images of the hand-written digits, with the labels known. So, this is a superviced learning problem. The first set is the training set. With this data we are going to make a model. The model can be checked with the test set. If the model also performs good on the test set, it indicates the model isn't overfit or overtrained. There are multiple solutions to this problem.

\subsection{Problem Solutions}
Distance-based classifier
An image exists out of 256 points. Those points or pixels have a value. Based on these values we can say something about an image, so called feature. First we are going to load all the images of digit 0 of the training set and extract the features. Than we can calculate the average values. We repeat this for every digit. For a following image, where we don't know the label of, we extract also the features and compare them with the values of the averages we calculate in the step before. The one with the least distance will be probably the correct digit.

The features are the center, the diameter, the radius and the standard deviation of distances from the center, but more and better features can be implemented. 

When the center feature is used for training the model and the train set is checked, the accuracy is 86\%. When the model is checked on the test set, the accuracy is lowered to 80\%, but this is expected to be lower. See ~\fig{cm} for the confusion matrix. The numbers on the diagonal from top left to bottom right are the correctly classified digits. Most digits are on that line, but it's not perfect and there are errors. Digit 0 and digit 6 are often confused. 

\begin{table}
	\begin{tabular}{l c r}
	178 & 0 & 2 & 3 & 1 & 3 & 7 & 0 & 3 & 0 \\
	0 & 120 & 0 & 0 & 3 & 0 & 0 & 2 & 2 & 5 \\
	3 & 0 & 69 & 3 & 3 & 0 & 2 & 1 & 0 & 0 \\
	2 & 0 & 6 & 61 & 0 & 6 & 0 & 0 & 6 & 0 \\
	4 & 0 & 8 & 1 & 69 & 3 & 2 & 5 & 3 & 8 \\
	2 & 0 & 1 & 8 & 0 & 38 & 1 & 0 & 3 0 \\
	23 & 1 & 0 & 0 & 1 & 1 & 78 & 0 & 0 & 0 \\
	1 & 0 & 2 & 0 & 1 & 0 & 0 & 50 & 0 & 5 \\
	10 & 0 & 13 & 1 & 0 & 0 & 0 & 0 & 73 & 2 \\
	1 & 0 & 0 & 2 & 8 & 4 & 0 & 6 & 2 & 68 \\
	\end{tabular}
	\caption{Confusion Matrix}
	\label{tab:cm}
\end{table}



Perceptron Algorithm
A perceptron is a small neural network. It's based on inputs, weights and outputs. The inputs are the pixels of a image. With the correct weight it's possible to predict the right output. The weights gives importance to a specific pixel. In our case there is just one output node which can have two values. 0 or 1, or true and false. We built 10 models and every model gives an answer to the question "Is this hand-written image a *digit*?" So after checking an image with the 10 models, we can say which number the digit is.

Also here the models makes errors, but in this situations other errors can be made. The Distance-based classifier can classify a digit wrong, but with the perceptron it can happen that the a digit could not be classified or a digit can be classified as multiple classes. This makes a comparion between the confusion matrices difficult. 

Logistic regression classifier




\end{document}
